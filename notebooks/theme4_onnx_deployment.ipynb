{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bibliothèque et téléchargement des données"
      ],
      "metadata": {
        "id": "NiDOFa_UD9Gt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_81O0AEdDTWX"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "print(\"Chargement des bibliothéques\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "kG5eL9AlEE4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = os.listdir(os.path.join(path, \"chest_xray\"))\n",
        "print(\"Directory contents:\", contents)"
      ],
      "metadata": {
        "id": "4FCG_YmPEJbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contents = os.listdir(os.path.join(path, \"chest_xray/train\"))\n",
        "print(\"Directory contents:\", contents)"
      ],
      "metadata": {
        "id": "Db-MQUq1EJ6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PARTIE 1"
      ],
      "metadata": {
        "id": "Vj-pUdYcEMMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = [\"chest_xray/train\", \"chest_xray/test\", \"chest_xray/val\"]\n",
        "\n",
        "for path_ in content:\n",
        "    dataset = torchvision.datasets.ImageFolder(os.path.join(path, path_))\n",
        "\n",
        "    classes = dataset.classes\n",
        "    lenght = len(dataset)\n",
        "    index = dataset.class_to_idx\n",
        "    targets = dataset.targets\n",
        "    stats = Counter(dataset.targets)\n",
        "    print(f\"{path_}\\n\")\n",
        "\n",
        "    for i in index.keys():\n",
        "        for j in stats.keys():\n",
        "            if index[i] == j:\n",
        "                print(f\"Class: {i}, Number of images: {stats[j]}, percentage: {stats[j]/lenght:.2%}\")\n",
        "\n",
        "    print(\"Number of images: \", lenght, \"\\nClasses: \", classes, \"\\nClass to index mapping: \", index, \"\\n\")\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(stats.keys(), stats.values())\n",
        "    plt.xticks(list(index.values()), list(index.keys()))\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Number of images\")\n",
        "    plt.title(f\"Distribution of classes in {path_}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.pie(stats.values(), labels=list(index.keys()), autopct=\"%1.1f%%\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "U6ogBsr0EOo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "db_path = os.path.join(path, 'chest_xray')\n",
        "train_normal = os.path.join(db_path, 'train', 'NORMAL')\n",
        "train_pneu = os.path.join(db_path, 'train', 'PNEUMONIA')\n",
        "image_hasard_sain = os.listdir(train_normal)[0]\n",
        "image_hasard_pneumo = os.listdir(train_pneu)[0]\n",
        "chemin_image_sain = os.path.join(train_normal, image_hasard_sain)\n",
        "chemin_image_pneumo = os.path.join(train_pneu, image_hasard_pneumo)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Image Normale\n",
        "plt.subplot(1, 2, 1) # 1 ligne, 2 colonnes, position 1\n",
        "plt.imshow(Image.open(chemin_image_sain), cmap='gray')\n",
        "plt.title(\"Poumon NORMAL\")\n",
        "\n",
        "# Image Pneumonie\n",
        "plt.subplot(1, 2, 2) # 1 ligne, 2 colonnes, position 2\n",
        "plt.imshow(Image.open(chemin_image_pneumo), cmap='gray')\n",
        "plt.title(\"Poumon PNEUMONIE\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NrajRYPjEQJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def print_stats(nom_dossier, db_path):\n",
        "    liste_moy = []\n",
        "    liste_ecart_type = []\n",
        "\n",
        "    fichiers = os.listdir(db_path)\n",
        "\n",
        "    for image in fichiers :\n",
        "        chemin_image = os.path.join(db_path, image)\n",
        "        img = Image.open(chemin_image)\n",
        "        px = np.array(img)\n",
        "        liste_moy.append(np.mean(px))\n",
        "        liste_ecart_type.append(np.std(px))\n",
        "\n",
        "    return {\n",
        "        'nom': nom_dossier,\n",
        "        'mean': np.mean(liste_moy),\n",
        "        'std': np.mean(liste_ecart_type),\n",
        "        'count': len(liste_moy)\n",
        "    }\n",
        "\n",
        "stats_sain = print_stats(\"SAIN\", train_normal)\n",
        "stats_pneu = print_stats(\"PNEUMONIE\", train_pneu)\n",
        "\n",
        "print(stats_sain)\n",
        "print(stats_pneu)\n"
      ],
      "metadata": {
        "id": "7mtMVanWETfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# On crée une liste avec nos deux dictionnaires de stats\n",
        "resultats = [stats_sain, stats_pneu]\n",
        "\n",
        "# Pandas transforme automatiquement une liste de dictionnaires en tableau !\n",
        "comparaison = pd.DataFrame(resultats)\n",
        "\n",
        "# On renomme les colonnes pour que ce soit joli\n",
        "comparaison.columns = ['Classe', 'Moyenne Luminosité', 'Contraste (Std)', 'Nombre Images']\n",
        "\n",
        "print(comparaison)"
      ],
      "metadata": {
        "id": "fEvxLHQPEU9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On convertit les moyennes (0-255) en format PyTorch (0-1)\n",
        "final_mean = (stats_sain['mean'] + stats_pneu['mean']) / 2 / 255\n",
        "final_std = (stats_sain['std'] + stats_pneu['std']) / 2 / 255\n",
        "\n",
        "print(f\"Mean pour Normalize: {final_mean:.4f}\")\n",
        "print(f\"Std pour Normalize: {final_std:.4f}\")"
      ],
      "metadata": {
        "id": "WT_5ci_eEWB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THEME 2"
      ],
      "metadata": {
        "id": "UacQ9fOLEXH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les transformers sont réutilisés dans la partie 7"
      ],
      "metadata": {
        "id": "q6OM04bwEbfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "#On redimensionne les images (224×224) et on les normalise\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomRotation(10), #permet de généraliser\n",
        "        transforms.RandomResizedCrop(224), #zoom etape par etape\n",
        "        transforms.ToTensor(), #gagner en temps de calcul\n",
        "        transforms.Normalize(mean=[final_mean], std=[final_std])\n",
        "    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[final_mean], std=[final_std])\n",
        "    ])\n",
        "\n",
        "print(\"transform train et test crée\")\n"
      ],
      "metadata": {
        "id": "TQ35qvVdEZpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#Permet que l'expérience soit reproductible\n",
        "#Sans cela, le hasard change à chaque fois et on ne peut pas comparer les résultats d'un jour à l'autre\n",
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True # Désactive les algos non-déterministes\n",
        "    torch.backends.cudnn.benchmark = False     # Évite les variations de performance\n",
        "\n",
        "fix_seed(42)"
      ],
      "metadata": {
        "id": "weEAI9s5EhEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = os.path.join(db_path, 'train')\n",
        "test_dataset = os.path.join(db_path, 'test')\n",
        "val_dataset = os.path.join(db_path, 'val')\n",
        "\n",
        "full_train = datasets.ImageFolder(os.path.join(db_path, \"train\"), transform=transform_train)\n",
        "train_clean = datasets.ImageFolder(os.path.join(db_path, \"train\"), transform=transform_test)\n",
        "full_test = datasets.ImageFolder(os.path.join(db_path, \"test\"), transform=transform_test)\n",
        "\n",
        "indices = torch.randperm(len(full_train), generator=torch.Generator().manual_seed(42))\n",
        "train_idx, val_idx = indices[:int(0.9*len(full_train))], indices[int(0.9*len(full_train)):]\n",
        "\n",
        "#Au lieu de charger 5000 images d'un coup on les envoies par petits paquets (Batches)\n",
        "train_loader = DataLoader(Subset(full_train, train_idx), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(full_test, batch_size = BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(ConcatDataset([datasets.ImageFolder(os.path.join(db_path, \"val\"), transform=transform_test),\n",
        "                                       Subset(train_clean, val_idx)]), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "EQ62Sc-3Eibu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "baseline_metrics = {\n",
        "    'mean': final_mean,\n",
        "    'std': final_std,\n",
        "    'classes': ['NORMAL', 'PNEUMONIA'],\n",
        "    'distribution': [len(os.listdir(train_normal)), len(os.listdir(train_pneu))]\n",
        "}\n",
        "\n",
        "torch.save(baseline_metrics, 'pneumonia_baseline_metrics.pt')\n",
        "print(\"Baseline sauvegardée pour l'étape 6\")"
      ],
      "metadata": {
        "id": "LI5dP6H5EkE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#On crée un réseau avec 4 blocs de convolution, de la Batch Normalization et du Dropout.\n",
        "#Convolution : Le modèle apprend à détecter des formes\n",
        "#Batch Normalization : Ça stabilise l'apprentissage\n",
        "#Dropout : On éteint la moitié des neurones au hasard pendant l'entraînement.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Bloc 1 : 224x224 -> 112x112\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Bloc 2 : 112x112 -> 56x56\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Bloc 3 : 56x56 -> 28x28\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Bloc 4 (Ajouté pour le budget paramètres) : 28x28 -> 14x14\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # MLP Final : Entrée 64 * 14 * 14 = 12 544\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 128) # Réduit à 128 pour l'efficacité\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x)))) # 4ème réduction\n",
        "\n",
        "        x = x.view(x.size(0), -1) # Flatten propre\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Le CNN est réalisé\")"
      ],
      "metadata": {
        "id": "6Kfa9pXKElrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SImpleCNN réutilisé à la partie 7"
      ],
      "metadata": {
        "id": "0FDjg8VcEm_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Le dataset a sûrement beaucoup plus de cas \"Pneumonie\" que \"Normal\".\n",
        "# Sans ces poids, le modèle choisirait la facilité en prédisant \"Pneumonie\" tout le temps.\n",
        "# On lui dit : \"Si tu te trompes sur un cas Normal, je te punis 2.8 fois plus fort\".\n",
        "# Adam : C'est un moteur intelligent qui ajuste la vitesse d'apprentissage tout seul.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "weights = torch.tensor([2.8, 1.0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "writer = SummaryWriter('runs/pneumonia_baseline_1')"
      ],
      "metadata": {
        "id": "BM5n3vdpErFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Nombre de paramètres : {total_params:,}\")\n",
        "# On devrait être autour de 1.2M - 1.5M selon les tailles des FC."
      ],
      "metadata": {
        "id": "BQa07IF8EsT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test rapide : Est-ce que les loaders existent ?\n",
        "try:\n",
        "    print(f\"Train loader : {type(train_loader)} - OK\")\n",
        "    print(f\"Val loader : {type(val_loader)} - OK\")\n",
        "    # On vérifie si on peut tirer un batch\n",
        "    images, labels = next(iter(train_loader))\n",
        "    print(f\"Batch testé : {images.shape}\")\n",
        "except NameError as e:\n",
        "    print(f\" Erreur : La variable n'existe pas. {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Autre erreur : {e}\")"
      ],
      "metadata": {
        "id": "jCtrjP_6Etvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    # Niveau 1 : Dans la fonction (4 espaces)\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    print(\"On lance le training\")\n",
        "    for epoch in range(epochs):\n",
        "        # Niveau 2 : Dans la boucle for (8 espaces)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        print(f\"\\nÉpoque {epoch+1}/{epochs}\")\n",
        "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            # Niveau 3 : Dans la boucle des images (12 espaces)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Retour au niveau 2 pour les calculs de fin d'époque\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = 100 * correct_train / total_train\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        # Mise à jour du dictionnaire history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"⭐ Nouveau meilleur modèle sauvegardé ! (Acc: {best_val_acc:.2f}%)\")\n",
        "\n",
        "    # Retour au niveau 1 : Fin de la fonction\n",
        "    writer.close()\n",
        "    print(\"\\nEntraînement terminé !\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "PTj6F-GsEwGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_CNN = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "# 2. On affiche les courbes avec Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Graphique de la Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_CNN['train_loss'], label='Train Loss')\n",
        "plt.plot(history_CNN['val_loss'], label='Val Loss')\n",
        "plt.title('Courbe de Perte (Loss)')\n",
        "plt.legend()\n",
        "\n",
        "# Graphique de l'Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_CNN['train_acc'], label='Train Acc')\n",
        "plt.plot(history_CNN['val_acc'], label='Val Acc')\n",
        "plt.title('Courbe de Précision (Accuracy)')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8EEg8PFGExry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_final(model, test_loader):\n",
        "    # Charger les meilleurs poids sauvegardés\n",
        "    # Pour voir les \"Learning Curves\". Si la courbe de Validation s'envole alors que le Train descend = KO\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Précision Finale sur le Test Set : {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Re-initialize test_loader correctly using the full_test dataset\n",
        "test_loader = DataLoader(full_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_final(model, test_loader)"
      ],
      "metadata": {
        "id": "W1NhUc_CEzjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En sauvegardant le meilleur score, on garde le \"meilleur de l'intelligence\" du modèle, même si l'epoch d'après il devient nul.\n",
        "\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': 10,\n",
        "    'loss': 0.33\n",
        "}\n",
        "torch.save(checkpoint, 'full_checkpoint.pth')"
      ],
      "metadata": {
        "id": "wDLRaEHRE04P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#On lance une évaluation sur le Test Set\n",
        "def plot_confusion_matrix(model, test_loader):\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                yticklabels=['NORMAL', 'PNEUMONIA'])\n",
        "    plt.xlabel('Prédiction du Modèle')\n",
        "    plt.ylabel('Vraie Étiquette')\n",
        "    plt.title('Matrice de Confusion')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=['NORMAL', 'PNEUMONIA']))\n",
        "\n",
        "\n",
        "plot_confusion_matrix(model, test_loader)"
      ],
      "metadata": {
        "id": "wbCzAH1XE3CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_finder(model, train_loader, criterion, optimizer, device):\n",
        "    lrs = []\n",
        "    losses = []\n",
        "    lr = 1e-6\n",
        "\n",
        "    print(\"Début du test de Learning Rate...\")\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Sécurité : Si la loss explose (devient 4x plus grande que le début), on arrête\n",
        "        if len(losses) > 0 and loss.item() > losses[0] * 4:\n",
        "            print(f\"Arrêt précoce : La loss explose à LR = {lr:.1e}\")\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        lrs.append(lr)\n",
        "        losses.append(loss.item())\n",
        "        lr *= 1.1\n",
        "        if lr > 1: break\n",
        "\n",
        "    # --- Bloc de Visualisation ---\n",
        "    if len(lrs) > 0:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(lrs, losses)\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel(\"Learning Rate (échelle log)\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"LR Finder : Cherchez la pente la plus raide\")\n",
        "        plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "        plt.show() # L'affichage final\n",
        "    else:\n",
        "        print(\"Erreur : Aucun point à afficher. Vérifiez votre loader.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xs137BGEE4o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model_resnet = models.resnet18(weights=None).to(device)\n",
        "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, 2).to(device)\n",
        "\n",
        "optimizer_test = torch.optim.Adam(model_resnet.parameters(), lr=1e-6)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "model_resnet = model_resnet.to(device)\n",
        "\n",
        "lr_finder(model_resnet, train_loader, criterion, optimizer_test, device)"
      ],
      "metadata": {
        "id": "z7COf9h6E6eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. RÉINITIALISATION\n",
        "model_resnet = models.resnet18(weights=None).to(device)\n",
        "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, 2).to(device)\n",
        "\n",
        "# 2. CONFIGURATION OPTIMISÉE\n",
        "# On utilise le LR trouvé : 1e-5\n",
        "optimizer = torch.optim.Adam(model_resnet.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights) # Poids de classe toujours actifs\n",
        "\n",
        "# 3. LANÇER L'ENTRAÎNEMENT\n",
        "history_resnet = train_model(model_resnet, train_loader, val_loader, criterion, optimizer, epochs=15)"
      ],
      "metadata": {
        "id": "QXbMdhmhE8d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# S'assurer que les lignes commencent bien au début (colonne 0)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# --- Graphique de la Loss ---\n",
        "plt.subplot(1, 2, 1)\n",
        "# On vérifie si history_simple existe avant de tracer\n",
        "if 'history' in locals():\n",
        "    plt.plot(history['train_loss'], label='SimpleCNN Train', linestyle='--', color='blue')\n",
        "if 'history_resnet' in locals():\n",
        "    plt.plot(history_resnet['train_loss'], label='ResNet18 Train', color='red')\n",
        "plt.title('Comparaison de la Perte (Loss)')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# --- Graphique de l'Accuracy ---\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'history' in locals():\n",
        "    plt.plot(history['val_acc'], label='SimpleCNN Val', linestyle='--', color='blue')\n",
        "if 'history_resnet' in locals():\n",
        "    plt.plot(history_resnet['val_acc'], label='ResNet18 Val', color='red')\n",
        "plt.title('Comparaison de la Précision (Accuracy)')\n",
        "plt.xlabel('Époques')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DjSoUT9UE993"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PARTIE 4"
      ],
      "metadata": {
        "id": "nhk2-SuuE_Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_onnx = \"/kaggle/input/models/thastraudo/model-onnx/onnx/default/1\"\n",
        "if path_onnx :\n",
        "    print(\"Accès à ONNX\")"
      ],
      "metadata": {
        "id": "TlsUFH3iFBQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "model_path = os.path.join(path_onnx, \"model_optimized.onnx\")\n",
        "\n",
        "try:\n",
        "    session = ort.InferenceSession(model_path)\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    print(\"Succès : Le modèle ONNX est chargé et la session est prête !\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement : {e}\")"
      ],
      "metadata": {
        "id": "s_99M3iMFDIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "#Inference logger\n",
        "#__init__ : C'est la préparation. On crée le fichier production_logs.csv\n",
        "#on écrit les titres des colonnes : timestamp, la classe trouvée, la confiance du modèle et le temps de calcul\n",
        "#log : C'est l'action d'écrire. À chaque fois qu'on appelle cette fonction, elle ouvre le fichier, ajoute une ligne à la fin\n",
        "#(mode 'a' pour \"append\"), et le referme.\n",
        "\n",
        "class InferenceLogger:\n",
        "    def __init__(self, log_file='production_logs.csv'):\n",
        "        self.log_file = log_file\n",
        "        # On crée le fichier et on écrit les titres des colonnes\n",
        "        with open(log_file, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['timestamp', 'pred_class', 'confidence', 'latency_ms'])\n",
        "\n",
        "    def log(self, pred_class, confidence, latency_ms):\n",
        "        # On ajoute une ligne à chaque fois qu'on fait une prédiction\n",
        "        with open(self.log_file, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([datetime.now().isoformat(), pred_class, confidence, latency_ms])\n",
        "\n",
        "# On crée l'objet logger\n",
        "logger = InferenceLogger()\n",
        "print(\"code run\")"
      ],
      "metadata": {
        "id": "TNfM22XkFEeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simulation de la vraie vie\n",
        "#On déclenche un chronomètre juste avant que l'image entre dans le modèle.\n",
        "#On calcule le temps en millisecondes (ms)\n",
        "#Le modèle donne des scores bruts, Le Softmax transforme ces chiffres en pourcentages\n",
        "#On envoie toutes ces infos (prédiction, confiance, temps) vers notre carnet de bord créé à l'étape 1.\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "production_preds_onnx = []\n",
        "\n",
        "print(\"Lancement de l'inférence optimisée (ONNX)...\")\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    # --- AJUSTEMENT DES DIMENSIONS ---\n",
        "    # Si l'image est en RGB (3 canaux) mais que ONNX veut du Gris (1 canal)\n",
        "    if images.shape[1] == 3:\n",
        "        # On calcule la moyenne des 3 canaux pour simuler du gris\n",
        "        # Shape passe de [Batch, 3, H, W] à [Batch, 1, H, W]\n",
        "        images_onnx = images.mean(dim=1, keepdim=True)\n",
        "    else:\n",
        "        images_onnx = images\n",
        "\n",
        "    # 1. Conversion du Tensor PyTorch en tableau NumPy\n",
        "    input_data = images_onnx.cpu().numpy()\n",
        "\n",
        "    # 2. Mesure de la latence\n",
        "    start = time.time()\n",
        "    # On lance l'inférence sur le moteur ONNX\n",
        "    outputs = session.run(None, {input_name: input_data})[0]\n",
        "    latency = (time.time() - start) * 1000 / len(images)\n",
        "\n",
        "    # 3. Conversion en probabilités (Softmax manuel)\n",
        "    exp_outputs = np.exp(outputs - np.max(outputs, axis=1, keepdims=True))\n",
        "    probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
        "\n",
        "    # 4. Récupération des prédictions et confiances\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    confs = np.max(probs, axis=1)\n",
        "\n",
        "    # 5. Logging\n",
        "    for pred, conf in zip(preds, confs):\n",
        "        logger.log(int(pred), float(conf), latency)\n",
        "        production_preds_onnx.append({'pred': int(pred), 'conf': float(conf), 'latency': latency})\n",
        "\n",
        "print(f\" {len(production_preds_onnx)} prédictions effectuées avec ONNX.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xorGiLTmFGDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#on crée le tableau de bord\n",
        "df_prod = pd.read_csv('production_logs.csv')\n",
        "\n",
        "# Calcul des KPI (Indicateurs Clés de Performance)\n",
        "avg_confidence = df_prod['confidence'].mean()\n",
        "latency_p95 = df_prod['latency_ms'].quantile(0.95) # Le \"pire\" cas pour 95% des requêtes\n",
        "pred_distribution = df_prod['pred_class'].value_counts()\n",
        "\n",
        "# Envoi vers TensorBoard\n",
        "writer.add_scalar('Production/Avg_Confidence', avg_confidence, 0)\n",
        "writer.add_scalar('Production/Latency_P95', latency_p95, 0)\n",
        "writer.add_histogram('Production/Confidences', df_prod['confidence'].values, 0)\n",
        "writer.close()\n",
        "\n",
        "print(f\"Simulation terminée : {len(production_preds_onnx)} prédictions loguées.\")\n",
        "print(f\"Confiance moyenne : {avg_confidence:.3f}\")\n",
        "print(f\"Latence P95 : {latency_p95:.2f} ms\")"
      ],
      "metadata": {
        "id": "ihpheA2vFHjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde de la Baseline\n",
        "baseline_production = {\n",
        "    'avg_confidence': avg_confidence,\n",
        "    'latency_p95': latency_p95,\n",
        "    'pred_distribution': dict(pred_distribution)\n",
        "}\n",
        "torch.save(baseline_production, 'baseline_production_metrics.pt')\n"
      ],
      "metadata": {
        "id": "w8gpeZClFJIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
